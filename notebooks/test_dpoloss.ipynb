{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1fed3d5-b682-4bf2-a70f-b60abe887d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd80634-a11f-4676-88ae-1c199200f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gonna try to make sure everything's good for DPO udpates in code, since it's a bit to run iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01d47e4-5700-4283-9aed-6db2bb53550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"../outputs/models/bagofwords/bowtiny_dpo/\")\n",
    "mod = AutoModelForCausalLM.from_pretrained(\"../outputs/models/bagofwords/bowtiny_dpo/\", device_map=0, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57ae6ab-ca35-4fc1-bacd-cd8536b937d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02183722-6de4-4b11-9ea7-9a6f0228b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_logps(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    average_log_prob: bool = False,\n",
    "    label_pad_token_id: int = -100,\n",
    "    is_encoder_decoder: bool = False,\n",
    ") -> torch.FloatTensor:\n",
    "\n",
    "    if logits.shape[:-1] != labels.shape:\n",
    "        raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "    if not is_encoder_decoder:\n",
    "        labels = labels[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "    loss_mask = labels != label_pad_token_id\n",
    "\n",
    "    # dummy token; we'll ignore the losses on these tokens later\n",
    "    labels[labels == label_pad_token_id] = 0\n",
    "\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * loss_mask).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85133f23-00f0-446b-8ebb-8a31b78d9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpstr = [\"Newcastle won by 0-3\", \"Newcastle won by 1-1\"]\n",
    "inps = tok(inpstr, return_tensors=\"pt\", padding=True).to(mod.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6a50044-087e-451d-bfe3-c37e90f6ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  4030, 24773,   351,    30,   321,    12,   246],\n",
       "        [    2,  4030, 24773,   351,    30,   112,    12,   288]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e5c18b2-e127-4a65-a6b9-ab421d452f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-35.2500, -31.6250], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = mod(**inps)\n",
    "get_batch_logps(outs.logits, inps.input_ids, average_log_prob=False, label_pad_token_id=tok.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b826faff-1abb-4e64-9ea6-927fd03aed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 50272])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68d871-74ab-4f50-bf76-2f1abd509479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
